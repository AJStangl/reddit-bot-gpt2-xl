{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "APBHA3k23ZdP"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!git clone https://github.com/Xirider/finetune-gpt2xl.git\n",
    "!chmod -R 777 finetune-gpt2xl/\n",
    "!pip install transformers\n",
    "!pip install wandb\n",
    "!pip install transformers[deepspeed]\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import random\n",
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import os"
   ],
   "metadata": {
    "id": "6d4KANdQc74G"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Add key if you wish to use wandb\n",
    "# os.environ[\"WANDB_API_KEY\"] = \"\""
   ],
   "metadata": {
    "id": "WV1N9NWX3e1-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def sort_files_by_name(target_dir):\n",
    "    files = os.listdir(target_dir)\n",
    "    files_with_prefix = [f for f in files if f.__contains__(\"raw_training\")]\n",
    "    sorted_files = sorted(files_with_prefix, key=lambda x: float(x.split('_')[-1].split('.txt')[0]), reverse=True)\n",
    "    return [os.path.join(target_dir, item) for item in sorted_files]"
   ],
   "metadata": {
    "id": "-EWs8QQnZ0nz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "out_model = \"\" # Adjust this path to reflect the directory where you want to save your model\n",
    "assert out_model != \"\", \"Please set the out_model variable to a valid path\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Adjust this path to reflect the directory where your training script is at\n",
    "target_dir = \"/content/drive\"\n",
    "\n",
    "target_file = sort_files_by_name(target_dir)\n",
    "\n",
    "print(target_file)\n",
    "\n",
    "d_lines = []\n",
    "for t_file in target_file:\n",
    "    with open(t_file, 'rb') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.decode('unicode_escape')\n",
    "            d_lines.append(line)"
   ],
   "metadata": {
    "id": "c-A8RedIa0Gy"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "__tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "sorted_lines = sorted(d_lines, key=len, reverse=True)\n",
    "data_lines = []\n",
    "encoded_tokens = []\n",
    "for line in tqdm(sorted_lines, total=len(sorted_lines)):\n",
    "        line = line.strip()\n",
    "        line = line.split(\"<|endoftext|>\")[0]\n",
    "        line += \"<|endoftext|>\"\n",
    "        if line.__contains__('[deleted]') or line.__contains__('[removed]'):\n",
    "            continue\n",
    "        encoded = __tokenizer.encode(line)\n",
    "        if len(encoded) > 1024:\n",
    "            continue\n",
    "        data_lines.append(line)\n",
    "        encoded_tokens.append(encoded)\n",
    "\n",
    "display(data_lines[:10])"
   ],
   "metadata": {
    "id": "fmfbKAkvb6Rp"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "generator = torch.Generator()\n",
    "\n",
    "generator.manual_seed(0)\n",
    "\n",
    "train_size = int(0.8 * len(data_lines))\n",
    "\n",
    "train_dataset_file, eval_dataset_file = random_split(list(data_lines), [train_size, len(data_lines) - train_size], generator=generator)\n",
    "\n",
    "random.shuffle(data_lines)"
   ],
   "metadata": {
    "id": "4HoWxmLRhaJI"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(f\"Train: {len(train_dataset_file)}\")\n",
    "print(f\"Eval: {len(eval_dataset_file)}\")\n",
    "print(f\"Total: {len(train_dataset_file)  + len(eval_dataset_file)}\")"
   ],
   "metadata": {
    "id": "rpaXYkibl_A-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "with open('/content/finetune-gpt2xl/train.csv', mode='w', encoding='utf-8') as csv_file:\n",
    "    fieldnames = ['text']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for line in train_dataset_file:\n",
    "        writer.writerow({'text': line})"
   ],
   "metadata": {
    "id": "_EhO8uQTdx1R"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%%bash\n",
    "\n",
    "head /content/finetune-gpt2xl/train.csv -n 5"
   ],
   "metadata": {
    "id": "s3rvGNL7463U"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "with open('/content/finetune-gpt2xl/validation.csv', mode='w', encoding='utf-8') as csv_file:\n",
    "    fieldnames = ['text']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for line in eval_dataset_file:\n",
    "        writer.writerow({'text': line})"
   ],
   "metadata": {
    "id": "X32m9gtN9cte"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%%bash\n",
    "\n",
    "head /content/finetune-gpt2xl/validation.csv -n 5"
   ],
   "metadata": {
    "id": "W_JG8A7C-iH3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!deepspeed --num_gpus=1 /content/finetune-gpt2xl/run_clm.py \\\n",
    "--deepspeed /content/finetune-gpt2xl/ds_config.json \\\n",
    "--model_name_or_path gpt2-xl \\\n",
    "--train_file /content/finetune-gpt2xl/train.csv \\\n",
    "--validation_file /content/finetune-gpt2xl/validation.csv \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--fp16 \\\n",
    "--overwrite_cache \\\n",
    "--evaluation_strategy \"steps\" \\\n",
    "--output_dir {out_model} \\\n",
    "--eval_steps 500 \\\n",
    "--num_train_epochs 1 \\\n",
    "--gradient_accumulation_steps 2 \\\n",
    "--per_device_train_batch_size 8 \\\n",
    "--per_device_eval_batch_size 8 \\\n",
    "--save_total_limit=2"
   ],
   "metadata": {
    "id": "bRt-RUH0-iDN"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# DO THIS IF YOU NEED TO CONTINUE TRAINING, SET THE  --resume_from_checkpoint /content/drive/MyDrive/RawData/gpt/big-bot-2/checkpoint-2000 to your expected path where you are saving checkpoints\n",
    "# !deepspeed --num_gpus=1 /content/finetune-gpt2xl/run_clm.py \\\n",
    "# --deepspeed /content/finetune-gpt2xl/ds_config.json \\\n",
    "# --model_name_or_path gpt2-xl \\\n",
    "# --resume_from_checkpoint /content/drive/MyDrive/RawData/gpt/big-bot-2/checkpoint-2000 \\\n",
    "# --train_file /content/finetune-gpt2xl/train.csv \\\n",
    "# --validation_file /content/finetune-gpt2xl/validation.csv \\\n",
    "# --do_train \\\n",
    "# --do_eval \\\n",
    "# --fp16 \\\n",
    "# --overwrite_cache \\\n",
    "# --evaluation_strategy \"steps\" \\\n",
    "# --output_dir {out_model} \\\n",
    "# --eval_steps 500 \\\n",
    "# --num_train_epochs 1 \\\n",
    "# --gradient_accumulation_steps 2 \\\n",
    "# --per_device_train_batch_size 8 \\\n",
    "# --per_device_eval_batch_size 8 \\\n",
    "# --save_total_limit=2"
   ],
   "metadata": {
    "id": "HYBXnoWchMQc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(out_model)\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2LMHeadModel.from_pretrained(out_model)\n",
    "\n",
    "special_tokens_dict = {\n",
    "    \"bos_token\": \"<|startoftext|>\",\n",
    "    \"eos_token\": \"<|endoftext|>\",\n",
    "    \"additional_special_tokens\": [\n",
    "        \"<|endoftext|>\",\n",
    "        \"<|startoftext|>\",\n",
    "        \"<|subreddit|>\",\n",
    "        \"<|title|>\",\n",
    "        \"<|text|>\",\n",
    "        \"<|context_level|>\",\n",
    "        \"<|comment|>\"\n",
    "    ]\n",
    "}\n",
    "model.save_pretrained(out_model)\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(device)\n",
    "\n",
    "print(\"model loaded\")"
   ],
   "metadata": {
    "id": "be5qIR5q-wCr"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# this is a single input batch with size 3\n",
    "texts = [\"<|startoftext|>\", \"<|startoftext|><|subreddit|>\", \"<|startoftext|><|subreddit|>AskReddit<|title|>\"] * 3\n",
    "\n",
    "encoding = tokenizer(texts, padding=True, return_tensors='pt').to(device)\n",
    "\n",
    "inputs = encoding['input_ids']\n",
    "attention_mask = encoding['attention_mask']\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(inputs=inputs, attention_mask=attention_mask, max_length=1024, repetition_penalty=1.1, num_return_sequences=1)\n",
    "    generated_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=False)\n",
    "    for i in range(len(texts)):\n",
    "        print(f\"{generated_texts[i]}\")"
   ],
   "metadata": {
    "id": "gg5sUUZVhOuS"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import runtime\n",
    "runtime.unassign()"
   ],
   "metadata": {
    "id": "wgmnkbDu43x4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "-jnOPGnf_0Y8"
   }
  }
 ]
}
